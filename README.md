# Backdoor Attack for Vision models 
Trustworthy and Explainable AI project



From our github repository, the #submission branch contains the final version of the code and the other files that are needed for running the code. 
All the content from the folder should be downloaded in a folder and they all should be at the same level (neither one of them should be place in subfolders). The two notebooks for the Resnet Model (resnet_cifar.ipynb) and for the Visual Transformer (vit_cifar.ipynb) can be ran directly by running all the cells at once (Run all). The paths for the trigger is the relative path of the trigger.jpg and the ones for the noised_trigger can be set to one of the two noised_trigger images in the repository (they are already set to noised_trigger_epoch_150.png). If one runs the noise.ipynb, all 4 noised_images (for 50, 100, 150 and 200) will be outputted and saved. 
All the libraries used and their versions can be found in the requirements.txt and these should be installed. The python version used was 3.11.11. 


Requirements.txt

This file contains all the requirements for the versions of the library used. One can install these by running pip3 install -r requirements.txt in the terminal. 


Utils.py


For this file, we used the libraries Pillow, random and torchvision. We gathered in this file the functions that we used for both ResNet18 and Vision Transformer models, which are the attack functions, the functions for creating the poisoned datasets and the transformation of the data before passing it to the models. 

For the function resize(), paste(), ToPILImage(), composite(), we used the documentation from https://pillow.readthedocs.io/en/stable/reference/Image.html. 


The small_trigger_attack function resizes the trigger function and then it places on the image from the dataset. It takes as arguments an image, that can be from the training or the testing set, the label of this image,the trigger image, the proportion of the trigger image size relative to the passed image and the x, y coordinates of where the trigger image needs to be placed (we also pass the alpha becuase we are using it for the next attacks and then we are also using these functions as parameters so we wanted them to have the same structure, but it should be kept as 0). We make a copy of the image, take its height and width, and used them to resize the trigger width and heigth by multiplying it to gamma. We then take the minimum between the resulted coordinates and the difference between the base image coordinates and the resulted coordinates to make sure that the trigger image does not go out of bounds. Then, we paste the image at the resulted coordinates on top of the base image, we convert it and return it with the poisoned label 99. 

The watermark_trigger function takes the same parameters as the previous function, but now the coordinates and the gamma should be kept at 0. The alpha represents the transperancy of the trigger image. We transform the image into an Image object if it is a tensor and we convert it to 'RGBA' in order to manipulate transperancy. We then resize the trigger image to the size of the passed image from the dataset and blend it. We return both the poisoned image and the labels. 

The noised_trigger_attack function recieves the same parameters as for the last two functions and behaves the same as the previous one. The only difference is that this function takes as parameter a noise tirggered generated by us instead of a normal image trigger. 

Afterwards, in the function posion_dataset, we create the poisoned dataset that we mainly used for the training set. It takes as parameters an image, its label, the attack that we want to poison the image with, the percentage that represents the amount of poisoned dataset in the entire dataset, and the arguments of the attack functions. We first sample randomly as many number of poisons as the percentage indicates (we multiply the percentage by the total number of samples in the set). Then we loop through the dataset and we check whether the index of the respective image is in the random sample. If it is, we are applying the attack to it and we add both the resulted image and the poisoned label to the dataset, and if we not we add the original data and label to the new created dataset. In the function poison_entire_dataset, we do the same as in the previous function, with the difference that we are applying the attack to the all data in the set. 

These functions are called inside the two jupyter notebooks that we have for each of the two models. 


Noise.ipynb

For this file, we use the libraries numpy, torch, copy, torchvision, tqdm, Pillow and matplotlib. Inside this file we are creating the noised trigger image that will be used in the noised trigger attack. 

At first, we load the trigger image, and then we get the shallow layers (first 5 layers as suggested in (Ning et al., 2021)) of a pre trained resent18 model and freeze the weights, since we are only intersted in feature extraction. 

We then use an autoencoder architecture for creating the noised trigger image. For the design of the encoder and decoder architecture, we took inspiration from (Ning et al., 2021).

We then create an instance of the autoencoder class, we set the Adam optimizers and the loss to the L1 loss, as suggested in the literature. 

Further, we preprocess the trigger image and transform it and set the parameters needed for training. Then, in the training loop, we pass the trigger to the autoencoder to get the noised image, and then we extract the features of both input and output. We calculate the loss, and then we backpropagate it and optimize. If the model has improved showing a better loss, then we assign this loss as the best one, we reset the counter and make a copy of the model's weights. If the model did not improve, we increase the counter and whether it is greater or equal to the patience (which is 5), and in this case, we do early stopping to prevent overfitting. At every 50 epochs, the noised trigger image is saved. For our project, we used 
the generated images at 100 and 150 epochs. (Inside the jupyter notebooks for the two models, one could change the noised_trigger path to the relative path of any of the images generated)


The models weights for the ResNet18 and vit_b_16 are stored in this Google Drive https://drive.google.com/drive/folders/1u9rEHv3B47BXLd4TIZp2j9F9NzOeX0kD?usp=sharing. In order to use them, they must be saved and passed as the weights parameter from the resnet18() and the vit_b_16() (in our notebooks they have been set to "DEFAULT", since we used pre-trained models for training and testing.)


Resnet_cifar.ipynb 


For this file, we import the json, torch, torchvision, Pillow, random and the functions from the utils.py file. At first we load the json file and set the model's parameters to the values set in the json file. Afterwards, we set a path in a 'data' folder to download the images from the CIFAR100 dataset, with the split for train true for the training set and false for the testing set. We then load the trigger and the noise trigger adding the relative paths to two variables (the triggers have to be downloaded form the repository and set at the same level as the rest of the files). We then define a dictionary for the posion_attack, so that they can all be passed through the model and have the evaluation made on them. However, due to memory capacity issues, when running the code, we had to comment out the attacks one by one, and run the code for one at a time (and this is also how we uploaded the models). We then set the size of the batches and the number of epochs and define a transform function to use on the data before passing it to the model. The implementation for this part was inspired from https://pytorch.org/vision/main/transforms.html. 

We then loop through the poison types, and define the needed trigger based on the attacks, customize the parameters needed based on the attacks type and create the datasets by passing the training data and labels, the attack type, the defined trigger and the customized arguments. We do the same for the testing dataset, and we transform the data. We then load the data in batches and create the resnet18 model. The weights are "default" because we are using the pre-trained model for this task, however when running the code, one could use the model weights shared through the link
https://drive.google.com/drive/folders/1u9rEHv3B47BXLd4TIZp2j9F9NzOeX0kD?usp=sharing. We then set the number of classes to 100 (specific to CIFAR100). We compute the loss using cross entropy, set an Adam optimizer, with a learning rate of 0.001 and then we train the model. We reset the gradient, backpropagate the gradients and update the model weights. We keep track of the loss and the accuracy. 
We then evaluate the model, in a similar way as we do for the training, but without updating the model weights. We then compute the accuracy based on the correct labels and the prediction labels and save the model. 

Vit_cifar.ipynb

This file has the same structure as the Resnet_cifar.ipynb. The only differences are that the model is vit_b_16() instead of the resnet18() one, and that we replace the classification head for the 100 classed specific to CIFAR100. In order to run the code, one can use the specific weights for the vit model from https://drive.google.com/drive/folders/1u9rEHv3B47BXLd4TIZp2j9F9NzOeX0kD?usp=sharing. 

config.json 

In this file, we set the (default) values for the parameters used throughout the project.  If the parameters are changed here, they will also be changed at inference time. 

This file has to be saved at the same level as the 2 jupyter notebooks, the noise.ipynb, the utils.py and the trigger images. 
