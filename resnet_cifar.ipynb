{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T12:06:37.424123Z",
     "start_time": "2025-04-10T12:06:37.419488Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "import random\n",
    "from utils import (\n",
    "    small_trigger_attack, watermark_trigger_attack, noised_trigger_attack,\n",
    "    poison_dataset, poison_entire_testset\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:31.192290Z",
     "start_time": "2025-04-10T12:08:31.187175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)"
   ],
   "id": "829d2c8e516f019a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/ygao/Documents/alexandria/RUG/trustworthy/finalproject/backdoor-trust/utils.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:34.804086Z",
     "start_time": "2025-04-10T12:08:34.801322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "alpha = config[\"alpha\"]\n",
    "x_coord_start = config[\"x_coord_start\"]\n",
    "y_coord_start = config[\"y_coord_start\"]\n",
    "gamma = config[\"gamma\"]\n",
    "poison_percentage = config[\"poison_percentage\"]"
   ],
   "id": "d63d73e93c14f0ef",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:38.826178Z",
     "start_time": "2025-04-10T12:08:35.362562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset cifar100\n",
    "data_path = \"./data/cifar100_data/\"\n",
    "train_data = datasets.CIFAR100(root=data_path, train=True, download=True)\n",
    "test_data = datasets.CIFAR100(root=data_path, train=False, download=True)\n",
    "train_images = [img for img, _ in train_data]\n",
    "train_labels = [label for _, label in train_data]\n",
    "test_images = [img for img, _ in test_data]\n",
    "test_labels = [label for _, label in test_data]"
   ],
   "id": "832761f53b5f79bc",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:38.832908Z",
     "start_time": "2025-04-10T12:08:38.829853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trigger\n",
    "trigger = Image.open(\"trigger.jpg\")\n",
    "noised_trigger = Image.open(\"noised_trigger_epoch_100.png\")\n"
   ],
   "id": "4b43a5393c7ea601",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:38.841177Z",
     "start_time": "2025-04-10T12:08:38.839034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# poison methods\n",
    "poison_attacks = {\n",
    "    \"small\": small_trigger_attack,\n",
    "    \"watermark\": watermark_trigger_attack,\n",
    "    \"noised\": noised_trigger_attack\n",
    "}"
   ],
   "id": "63b92c0fdcfc0ae8",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:43.433029Z",
     "start_time": "2025-04-10T12:08:43.430486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training param\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ],
   "id": "46e0c4ed7dcc22f7",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:08:44.865486Z",
     "start_time": "2025-04-10T12:08:44.824946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loop through poison types\n",
    "for poison_type, attack_fn in poison_attacks.items():\n",
    "    print(f\"\\n**** Training ResNet18 with {poison_type} trigger\")\n",
    "\n",
    "    # poison data and test set\n",
    "    trigger_img = noised_trigger if poison_type == \"noised\" else trigger\n",
    "\n",
    "    # Customize parameters based on attack type\n",
    "    attack_kwargs = {}\n",
    "    if poison_type == \"small\":\n",
    "        attack_kwargs = {\n",
    "            \"gamma\": gamma,\n",
    "            \"x_coord_start\": x_coord_start,\n",
    "            \"y_coord_start\": y_coord_start\n",
    "        }\n",
    "    else:  # \"watermark\" or \"noised\"\n",
    "        attack_kwargs = {\n",
    "            \"alpha\": alpha\n",
    "        }\n",
    "\n",
    "    poisoned_train, poisoned_train_labels = poison_dataset(\n",
    "        train_images, train_labels, attack_fn, trigger_img, poison_percentage,\n",
    "        **attack_kwargs\n",
    "    )\n",
    "\n",
    "    poisoned_test, poisoned_test_labels = poison_entire_testset(\n",
    "        test_images, test_labels, attack_fn, trigger_img,\n",
    "        **attack_kwargs\n",
    "    )\n",
    "\n",
    "    # transform\n",
    "    train_data_transformed = []\n",
    "    for img, label in zip(poisoned_train, poisoned_train_labels):\n",
    "        try:\n",
    "            train_data_transformed.append((transform(img), label))\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming training image: {e}\")\n",
    "            continue\n",
    "\n",
    "    test_data_transformed = []\n",
    "    for img, label in zip(poisoned_test, poisoned_test_labels):\n",
    "        try:\n",
    "            test_data_transformed.append((transform(img), label))\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming test image: {e}\")\n",
    "            continue\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model\n",
    "    model = resnet18(weights=\"DEFAULT\").to(device)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 100).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.5f}, Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # save\n",
    "    model_path = f\"resnet18_cifar100_poison_{poison_type}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    print(f\"Finished training on poison: {poison_type}\")\n"
   ],
   "id": "b0cb8b9aacf3659c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Training ResNet18 with small trigger\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 21\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# \"watermark\" or \"noised\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     attack_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malpha\u001B[39m\u001B[38;5;124m\"\u001B[39m: alpha\n\u001B[1;32m     19\u001B[0m     }\n\u001B[0;32m---> 21\u001B[0m poisoned_train, poisoned_train_labels \u001B[38;5;241m=\u001B[39m poison_dataset(\n\u001B[1;32m     22\u001B[0m     train_images, train_labels, attack_fn, trigger_img, poison_percentage,\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattack_kwargs\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     26\u001B[0m poisoned_test, poisoned_test_labels \u001B[38;5;241m=\u001B[39m poison_entire_testset(\n\u001B[1;32m     27\u001B[0m     test_images, test_labels, attack_fn, trigger_img,\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattack_kwargs\n\u001B[1;32m     29\u001B[0m )\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# transform\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/alexandria/RUG/trustworthy/finalproject/backdoor-trust/utils.py:51\u001B[0m, in \u001B[0;36mpoison_dataset\u001B[0;34m(dataset, labels, attack_fn, trigger, percentage, **attack_kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (image, label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(dataset, labels)):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m poison_indices:\n\u001B[0;32m---> 51\u001B[0m         poisoned_img, poisoned_lbl \u001B[38;5;241m=\u001B[39m attack_fn(image, label, trigger, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattack_kwargs)\n\u001B[1;32m     52\u001B[0m         poisoned_data\u001B[38;5;241m.\u001B[39mappend(poisoned_img)\n\u001B[1;32m     53\u001B[0m         poisoned_labels\u001B[38;5;241m.\u001B[39mappend(poisoned_lbl)\n",
      "File \u001B[0;32m~/Documents/alexandria/RUG/trustworthy/finalproject/backdoor-trust/utils.py:8\u001B[0m, in \u001B[0;36msmall_trigger_attack\u001B[0;34m(image, trigger_label, trigger, gamma, x_coord_start, y_coord_start)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msmall_trigger_attack\u001B[39m(image, trigger_label, trigger, gamma, x_coord_start, y_coord_start, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m----> 8\u001B[0m     base_image \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mToPILImage()(image)\n\u001B[1;32m      9\u001B[0m     base_image \u001B[38;5;241m=\u001B[39m base_image\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m     base_width, base_height \u001B[38;5;241m=\u001B[39m base_image\u001B[38;5;241m.\u001B[39msize\n",
      "File \u001B[0;32m~/anaconda3/envs/finalproject/lib/python3.11/site-packages/torchvision/transforms/transforms.py:234\u001B[0m, in \u001B[0;36mToPILImage.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    226\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    232\u001B[0m \n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mto_pil_image(pic, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode)\n",
      "File \u001B[0;32m~/anaconda3/envs/finalproject/lib/python3.11/site-packages/torchvision/transforms/functional.py:268\u001B[0m, in \u001B[0;36mto_pil_image\u001B[0;34m(pic, mode)\u001B[0m\n\u001B[1;32m    266\u001B[0m     pic \u001B[38;5;241m=\u001B[39m pic\u001B[38;5;241m.\u001B[39mnumpy(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pic, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be Tensor or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;66;03m# if 2D image, add channel dimension (HWC)\u001B[39;00m\n\u001B[1;32m    272\u001B[0m     pic \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(pic, \u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>."
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T11:44:51.882552Z",
     "start_time": "2025-04-10T11:44:51.881195Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1ff341143832172e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "623b2723bcc319b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
