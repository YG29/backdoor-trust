{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:44.071116Z",
     "start_time": "2025-04-10T10:18:42.715214Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_16\n",
    "from PIL import Image\n",
    "import random\n",
    "from utils import (\n",
    "    small_trigger_attack, watermark_trigger_attack, noised_trigger_attack,\n",
    "    poison_dataset, poison_entire_testset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63d73e93c14f0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:45.389568Z",
     "start_time": "2025-04-10T10:18:45.386537Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "alpha = config[\"alpha\"]\n",
    "x_coord_start = config[\"x_coord_start\"]\n",
    "y_coord_start = config[\"y_coord_start\"]\n",
    "gamma = config[\"gamma\"]\n",
    "poison_percentage = config[\"poison_percentage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832761f53b5f79bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:49.927059Z",
     "start_time": "2025-04-10T10:18:46.059648Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "/usr/lib64/python3.11/tarfile.py:2283: RuntimeWarning: The default behavior of tarfile extraction has been changed to disallow common exploits (including CVE-2007-4559). By default, absolute/parent paths are disallowed and some mode bits are cleared. See https://access.redhat.com/articles/7004769 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dataset cifar\n",
    "data_path = \"./data/cifar100_data/\"\n",
    "train_data = datasets.CIFAR100(root=data_path, train=True, download=True)\n",
    "test_data = datasets.CIFAR100(root=data_path, train=False, download=True)\n",
    "train_images = [img for img, _ in train_data]\n",
    "train_labels = [label for _, label in train_data]\n",
    "test_images = [img for img, _ in test_data]\n",
    "test_labels = [label for _, label in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b43a5393c7ea601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:51.551209Z",
     "start_time": "2025-04-10T10:18:51.542138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trigger\n",
    "trigger = Image.open(\"trigger.jpg\")\n",
    "noised_trigger = Image.open(\"noised_trigger_epoch_150.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b92c0fdcfc0ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:19:12.633195Z",
     "start_time": "2025-04-10T10:19:12.631082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Poison methods\n",
    "poison_attacks = {\n",
    "    \"small\": small_trigger_attack,\n",
    "    #\"watermark\": watermark_trigger_attack,\n",
    "    #\"noised\": noised_trigger_attack\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e0c4ed7dcc22f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:19:31.230270Z",
     "start_time": "2025-04-10T10:19:31.227213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training param\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cb8b9aacf3659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Training VIT with small trigger\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(poisoned_train, poisoned_train_labels):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         train_data_transformed.append((\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m, label))\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError transforming training image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torchvision/transforms/functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loop through poison types\n",
    "for poison_type, attack_fn in poison_attacks.items():\n",
    "    print(f\"\\n**** Training VIT with {poison_type} trigger\")\n",
    "\n",
    "    # Poison data and test set\n",
    "    trigger_img = noised_trigger if poison_type == \"noised\" else trigger\n",
    "\n",
    "    # Customize parameters based on attack type\n",
    "    attack_kwargs = {}\n",
    "    if poison_type == \"small\":\n",
    "        attack_kwargs = {\n",
    "            \"gamma\": gamma,\n",
    "            \"x_coord_start\": x_coord_start,\n",
    "            \"y_coord_start\": y_coord_start\n",
    "        }\n",
    "    else:  # \"watermark\" or \"noised\"\n",
    "        attack_kwargs = {\n",
    "            \"alpha\": alpha\n",
    "        }\n",
    "\n",
    "    poisoned_train, poisoned_train_labels = poison_dataset(\n",
    "        train_images, train_labels, attack_fn, trigger_img, poison_percentage,\n",
    "        **attack_kwargs\n",
    "    )\n",
    "\n",
    "    poisoned_test, poisoned_test_labels = poison_entire_testset(\n",
    "        test_images, test_labels, attack_fn, trigger_img,\n",
    "        **attack_kwargs\n",
    "    )\n",
    "\n",
    "    # Transform the data\n",
    "    train_data_transformed = []\n",
    "    for img, label in zip(poisoned_train, poisoned_train_labels):\n",
    "        try:\n",
    "            train_data_transformed.append((transform(img), label))\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming training image: {e}\")\n",
    "            continue\n",
    "\n",
    "    test_data_transformed = []\n",
    "    for img, label in zip(poisoned_test, poisoned_test_labels):\n",
    "        try:\n",
    "            test_data_transformed.append((transform(img), label))\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming test image: {e}\")\n",
    "            continue\n",
    "\n",
    "    train_loader = DataLoader(train_data_transformed, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data_transformed, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model where we get the input features of the head and replace with new linear layer\n",
    "    model = vit_b_16(weights=\"DEFAULT\").to(device)\n",
    "    in_features = model.heads.head.in_features  \n",
    "    model.heads.head = nn.Linear(in_features, 100).to(device)  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.5f}, Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n",
    "\n",
    "\n",
    "    # Saving the model \n",
    "    model_path = f\"vit16_cifar100_poison_{poison_type}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    print(f\"Finished training on poison: {poison_type}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
